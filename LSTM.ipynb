{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07efb74f-4b36-420f-bd11-3a41e4a3169a",
   "metadata": {},
   "source": [
    "# Coarse-grained Preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23825220-5a48-44fb-9b04-5fedc429a5cc",
   "metadata": {},
   "source": [
    "## Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0c4209b-b4c7-4764-8a13-aaa8d3b33b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_excel\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\" \n",
    "import keras\n",
    "def parse(x):\n",
    "    return datetime.strptime(x,'%Y%m%d')\n",
    "def get_dataframe(data_type,date_no):\n",
    "    path=f'{data_type}/{date_no}'\n",
    "    data=pd.read_excel(path)\n",
    "    data['Date'] = pd.to_datetime(data['Date'], format='%Y%m%d')\n",
    "    pd.set_option('future.no_silent_downcasting', True)\n",
    "    data.replace('/', np.nan, inplace=True)\n",
    "    data.set_index('Date', inplace=True)\n",
    "    os.makedirs(f'data/new_data/{data_type}', exist_ok=True)\n",
    "    data.describe().to_csv(f'data/new_data/{data_type}/{date_no}_statistic.csv',encoding='utf-8-sig')\n",
    "    print(data.index)\n",
    "    output_dir = f'data_set/{data_type}'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    data.to_csv(f'data_set/{data_type}/processed_{date_no}.csv')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87cc650f-0a12-4fbf-b6c9-7c11a43ede67",
   "metadata": {},
   "source": [
    "## Feature Selection\n",
    "NOTE:it's not simply PCA.\n",
    "We plan to try 5 methods of supervised feature selection,namely:\n",
    "1. Filter Method:\n",
    "   \n",
    "    a. Pearson's Coefficient\n",
    "   \n",
    "    b. Chi Squared\n",
    "   \n",
    "    c. ANOVA Coefficient\n",
    "   \n",
    "3. Wrapper Method:\n",
    "   \n",
    "    a. Recursive Feature Elimination*\n",
    "   \n",
    "    b. Genetic Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59659884-5775-46de-9d5a-2e09cb187fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "# input split\n",
    "def my_rfe(df):\n",
    "    df_d = pd.DataFrame()\n",
    "    for feature in df.columns:\n",
    "        kmeans = KMeans(n_clusters=8, random_state=0)\n",
    "        df_d[feature + '_binned'] = kmeans.fit_predict(df[[feature]])\n",
    "    X=df_d.drop(columns=['CGM (mg / dl)'+ '_binned'],axis=1)\n",
    "    y=df_d['CGM (mg / dl)'+ '_binned'] \n",
    "    rfe=RFE(estimator=DecisionTreeClassifier(),n_features_to_select=4)\n",
    "    rfe.fit(X,y)\n",
    "    for i,col in zip(range(X.shape[1]),X.columns):\n",
    "        print(f\"{col} selected={rfe.support_[i]} rank={rfe.ranking_[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c69a3d4-0612-4b10-b1bc-7c2a76a5baa4",
   "metadata": {},
   "source": [
    "### Sample data Analysis(RFE)\n",
    "```python\n",
    "feature_selection_info = [\n",
    "    {'feature': 'CBG (mg / dl)_binned', 'selected': True, 'rank': 1},\n",
    "    {'feature': 'Blood Ketone (mmol / L)_binned', 'selected': False, 'rank': 4},\n",
    "    {'feature': 'CSII - bolus insulin (Novolin R, IU)_binned', 'selected': True, 'rank': 1},\n",
    "    {'feature': 'CSII - basal insulin (Novolin R, IU / H)_binned', 'selected': True, 'rank': 1},\n",
    "    {'feature': 'IU- s.c._binned', 'selected': True, 'rank': 1},\n",
    "    {'feature': 'IU- i.v._binned', 'selected': False, 'rank': 3},\n",
    "    {'feature': 'Non-insulin hypoglycemic agents - mg_binned', 'selected': False, 'rank': 2},\n",
    "    {'feature': 'Dietary calories_binned', 'selected': True, 'rank': 1},\n",
    "]\n",
    "```\n",
    "From above ,we can omit the 3 features:\n",
    "1. ```Blood Ketone (mmol / L)```,\n",
    "2. ```IU- i.v.```,\n",
    "3. ```Non-insulin hypoglycemic agents - mg``` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8c4f13-7639-4311-a799-1ab286fc5a54",
   "metadata": {},
   "source": [
    "## Fine-grained preprocessing\n",
    "1. Use a coarse hashmap derived from the manual recorded csv file to calculate the total calories for a meal.\n",
    "2. Extract number from ```Insulin dose - s.c.```,```Insulin dose - i.v.```,```Non-insulin hypoglycemic agents```\n",
    "3. Padding the empty cell in ```CSII - basal insulin (Novolin R, IU / H)``` columns with the very above non-empty cell.\n",
    "4. The ```CGM``` is the variables we want to predict,so remove other irrelavant columns(10, 11, 12, 13, 14, 15,16,17)\n",
    "5. If the data is unavailable ,fill in it with the nearly average value of calories in a meal.\n",
    "### NOTE:\n",
    "1. The scaler's dimentionality must match the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4cb08293-fa3f-43cd-975c-16186baf4715",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler,LabelEncoder,MultiLabelBinarizer\n",
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "import re\n",
    "import warnings\n",
    "import csv\n",
    "warnings.filterwarnings(\"ignore\", message=\"All-NaN slice encountered\")\n",
    "#转成有监督数据\n",
    "def series_to_supervised(data, n_in=1, n_out=1, n_lag=1,dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    #数据序列(也将就是input) input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j + n_lag, i)) for j in range(n_vars)]\n",
    "        #预测数据（input对应的输出值） forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j + n_lag)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j + n_lag, i)) for j in range(n_vars)]\n",
    "    #拼接 put it all together\n",
    "    agg = concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # 删除值为NAN的行 drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg\n",
    "hash_table = {}\n",
    "with open('ref_calories.csv', newline='') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    for row in reader:\n",
    "        # 使用第一列作为键，第二列作为值，填充哈希表\n",
    "        if len(row) >= 2:  # 确保行中至少有两列\n",
    "            key = row[0]\n",
    "            value = row[1]\n",
    "            hash_table[key] = value\n",
    "def extract_dietary_words(text):\n",
    "    if pd.notna(text):\n",
    "        res=0.0\n",
    "        phrases = re.findall(r'\\b[A-Za-z\\s]+(?=\\s\\d+ g|\\b)', text)\n",
    "        phrases = [phrase.strip(' g\\n') for phrase in phrases]\n",
    "        numbers = re.findall(r'\\b\\d+\\b', text)\n",
    "        for i, phrase in enumerate(phrases):\n",
    "            if phrase=='':\n",
    "                break\n",
    "            if i>=len(numbers):\n",
    "                break\n",
    "            res +=int(numbers[i]) * float(hash_table.get(phrase,2.0))#phrases_stripped\n",
    "        return res#', '.join(phrases_stripped[:-1]) + phrases_stripped[-1]\n",
    "    else:\n",
    "        return np.nan\n",
    "def extract_and_sum_numbers(s):\n",
    "    try:\n",
    "        numbers = re.findall(r'(\\d+)', s)\n",
    "        res =sum(int(num) for num in numbers)\n",
    "        return float(res)\n",
    "    except TypeError:\n",
    "        return 0.0\n",
    "def fine_preprocess(data_type,date_no,n_lag=1,type='I'):\n",
    "    # 数据预处理：加载数据集\n",
    "    path = f'data_set/{data_type}/processed_{date_no}.csv'\n",
    "    dataset = read_csv(path, header=0, index_col=0)\n",
    "    # 将 'data not available' 替换为 NaN\n",
    "    dataset.loc[dataset['Dietary intake'] == 'data not available', 'Dietary intake'] = '258.44'\n",
    "    dataset.loc[dataset['CSII - basal insulin (Novolin R, IU / H)']=='temporarily suspend insulin delivery','CSII - basal insulin (Novolin R, IU / H)']=0.0\n",
    "    dataset.loc[dataset['CSII - bolus insulin (Novolin R, IU)']=='temporarily suspend insulin delivery','CSII - bolus insulin (Novolin R, IU)']=0.0\n",
    "    \n",
    "    values = dataset.values\n",
    "    s_c_col = dataset.iloc[:, 5].astype(str)\n",
    "    n_ha_col = dataset.iloc[:, 6].apply(extract_and_sum_numbers).astype(str)\n",
    "    i_v_col = dataset.iloc[:, 9].astype(str)\n",
    "\n",
    "    # Extract the IU values using regular expressions\n",
    "    #Non-insulin hypoglycemic agents\n",
    "    s_c_iu = s_c_col.str.extract(r'(\\d+)\\s*IU', expand=False)\n",
    "    i_v_iu = i_v_col.str.extract(r'(\\d+)\\s*IU', expand=False)\n",
    "    # Convert extracted values to numeric\n",
    "    s_c_iu = pd.to_numeric(s_c_iu, errors='coerce')\n",
    "    i_v_iu = pd.to_numeric(i_v_iu, errors='coerce')\n",
    "    n_ha_iu = pd.to_numeric(n_ha_col, errors='coerce')\n",
    "    # Add the extracted values back to the DataFrame\n",
    "    dataset['IU- s.c.'] = s_c_iu\n",
    "    dataset['IU- i.v.'] = i_v_iu #Really Omit!\n",
    "    #dataset['Non-insulin hypoglycemic agents - mg']=n_ha_iu #Really omit!\n",
    "    dataset.drop('Insulin dose - s.c.',axis=1,inplace=True)\n",
    "    dataset.drop('Insulin dose - i.v.',axis=1,inplace=True)\n",
    "    dataset.drop('Non-insulin hypoglycemic agents',axis=1,inplace=True)\n",
    "    #dataset.drop('CBG (mg / dl)',axis=1,inplace=True)\n",
    "    dataset.drop('IU- s.c.',axis=1,inplace=True)\n",
    "    dataset.drop('IU- i.v.',axis=1,inplace=True)\n",
    "    try:\n",
    "        dataset.drop('饮食',axis=1,inplace=True) \n",
    "    except KeyError: \n",
    "        dataset.drop('进食量',axis=1,inplace=True)\n",
    "    dataset.drop('Blood Ketone (mmol / L)',axis=1,inplace=True)#Really omit\n",
    "    dataset['Dietary calories'] = dataset['Dietary intake'].apply(extract_dietary_words)\n",
    "    #print(dataset.describe())\n",
    "    dataset.drop('Dietary intake',axis=1,inplace=True)\n",
    "    # Assuming dataset is your DataFrame\n",
    "    dataset['CSII - basal insulin (Novolin R, IU / H)'] = dataset['CSII - basal insulin (Novolin R, IU / H)'].ffill(limit=12).astype(float)\n",
    "    dataset.fillna(0, inplace=True)\n",
    "    #dataset.drop('CSII - basal insulin (Novolin R, IU / H)',axis=1,inplace=True)\n",
    "    #print(dataset)\n",
    "    my_rfe(dataset)\n",
    "    csv_file_path = 'output.csv'\n",
    "    dataset.to_csv(csv_file_path, index=False)\n",
    "    values = dataset.values\n",
    "    #保证为float ensure all data is float\n",
    "    #values = values.astype('float32')\n",
    "    #归一化 normalize features\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaled = scaler.fit_transform(values)\n",
    "    reframed = series_to_supervised(scaled, 1, 1,n_lag)\n",
    "    reframed.drop(reframed.columns[[6,7,8,9]], axis=1, inplace=True)\n",
    "    #reduce_dimension(reframed)\n",
    "    #print(reframed)\n",
    "    return reframed,scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9e93e7-4cf9-45d6-946a-b92adddf1bf4",
   "metadata": {},
   "source": [
    "The best split train-test retio is 4:1,I guess. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "708d0577-e61d-4c5e-9c43-cf31e64bd117",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(reframed):\n",
    "    #数据准备\n",
    "    #把数据分为训练数据和测试数据 split into train and test sets\n",
    "    values = reframed.values\n",
    "    #拿一年的时间长度训练\n",
    "    #print(reframed.shape[0])\n",
    "    n_train_quarters = int(reframed.shape[0]*0.9)\n",
    "    #划分训练数据和测试数据\n",
    "    train = values[:n_train_quarters, :]\n",
    "    test = values[n_train_quarters:, :]\n",
    "    #拆分输入输出 split into input and outputs\n",
    "    train_X, train_y = train[:, :-1], train[:, -1]\n",
    "    test_X, test_y = test[:, :-1], test[:, -1]\n",
    "    #reshape输入为LSTM的输入格式 reshape input to be 3D [samples, timesteps, features]\n",
    "    train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\n",
    "    test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))\n",
    "    #print ('train_x.shape, train_y.shape, test_x.shape, test_y.shape')\n",
    "    #print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)\n",
    "    return train_X,train_y,test_X,test_y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0d342f-e89e-4f1c-b1d2-d6fbd7568fb3",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f569a383-7a7a-44df-b759-62dc7884c39e",
   "metadata": {},
   "source": [
    "This is because tensorflow can't be directly imported in python.And keras's BACKEND env should be assigned as one of the following :```tensorflow```,```fax```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7365fb68-c099-42f2-b36a-526ac32997d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ecc253b-d042-4491-9d91-eb4466e495f7",
   "metadata": {},
   "source": [
    "## Training\n",
    "### NOTE:\n",
    "1. the batch size must be a power of 2.\n",
    "\n",
    "Best hyperparameter ever:\n",
    "- optimizer:adam,(nadam isn't that bad,but others suffer)\n",
    "- epochs:20\n",
    "- batch_size:16\n",
    "- layers of LSTM:100\n",
    "- layers of Dense must be 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46a3467e-b585-4557-8848-9bdafd96d578",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Input\n",
    "from keras.layers import LSTM\n",
    "from matplotlib import pyplot\n",
    "from keras.optimizers import Adam\n",
    "##模型定义 design network\n",
    "def train(data_type,date_no,train_X,train_y,test_X,test_y,n_lag=1):\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(train_X.shape[1], train_X.shape[2])))\n",
    "    model.add(LSTM(100))\n",
    "    model.add(Dense(1))\n",
    "    learning_rate = 0.001\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(loss='mae', optimizer='adam')\n",
    "    #模型训练 fit network\n",
    "    history = model.fit(train_X, train_y, epochs=20, batch_size=16, validation_data=(test_X, test_y), verbose=0,\n",
    "                    shuffle=False)\n",
    "    #输出 plot history\n",
    "    pyplot.plot(history.history['loss'], label='train')\n",
    "    pyplot.plot(history.history['val_loss'], label='test')\n",
    "    pyplot.legend()\n",
    "    pyplot.savefig(f'data_set/trained_{data_type}_{date_no}_{n_lag}.png')\n",
    "    pyplot.show()\n",
    "    pyplot.close()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8884b6d6-a947-4b39-85c8-b22ce347a970",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1eb51434-aaff-4a1e-beb9-befc855939ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import concatenate\n",
    "def predict(data_type,date_no,model,scaler,train_X,train_y,test_X,test_y,n_lag=1):\n",
    "    #进行预测 make a prediction\n",
    "    #print(train_X.shape,train_y.shape,test_X.shape,test_y.shape)\n",
    "    yhat = model.predict(test_X)\n",
    "    test_X = test_X.reshape((test_X.shape[0], test_X.shape[2]))\n",
    "    #预测数据逆缩放 invert scaling for forecast\n",
    "    inv_yhat = concatenate((yhat, test_X[:, 1:]), axis=1)\n",
    "    #print('inv_yhat')\n",
    "    #print(inv_yhat.shape)\n",
    "    #print(scaler.n_features_in_)\n",
    "    inv_yhat = scaler.inverse_transform(inv_yhat)\n",
    "    inv_yhat = inv_yhat[:, 0]\n",
    "    inv_yhat = np.array(inv_yhat)\n",
    "    #真实数据逆缩放 invert scaling for actual\n",
    "    test_y = test_y.reshape((len(test_y), 1))\n",
    "    inv_y = concatenate((test_y, test_X[:, 1:]), axis=1)\n",
    "    inv_y = scaler.inverse_transform(inv_y)\n",
    "    inv_y = inv_y[:, 0]\n",
    "\n",
    "    #画出真实数据和预测数据\n",
    "    pyplot.plot(inv_yhat,label='prediction')\n",
    "    pyplot.plot(inv_y,label='true')\n",
    "    pyplot.legend()\n",
    "    pyplot.savefig(f'predict_samples')\n",
    "    pyplot.savefig(f'data_set/{data_type}/figures/{date_no}_{n_lag}.png')\n",
    "    pyplot.show()\n",
    "    pyplot.close()\n",
    "    return inv_y, inv_yhat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119d72e6-b529-46ac-94d2-6e692a686d7a",
   "metadata": {},
   "source": [
    "# Calculate RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0cfaa501-01ea-4fc1-bba7-a0a16976c945",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "# calculate RMSE\n",
    "def cal_rmse(inv_y, inv_yhat):\n",
    "    rmse = sqrt(mean_squared_error(inv_y, inv_yhat))\n",
    "    print('Test RMSE: %.3f' % rmse)\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fe25d7-a05a-4fdf-a744-c7c49732f937",
   "metadata": {},
   "source": [
    "## Cross Validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc615f46-d009-4f8a-9b99-bc0d5f9b1198",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "def roc_auc(y,yhat):\n",
    "    # 计算AUC-ROC\n",
    "    roc_auc = roc_auc_score(y, yhat)\n",
    "    fpr, tpr, _ = roc_curve(y, yhat)\n",
    "    roc_auc_value = auc(fpr, tpr)\n",
    "\n",
    "    print(f'AUC-ROC: {roc_auc}')\n",
    "\n",
    "    # 绘制ROC曲线\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc_value:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5edb8e-6ea3-491e-9c81-d69a62037d10",
   "metadata": {},
   "source": [
    "## Enumerate all files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebc7863-0d1f-4dc7-a6d5-757367e83a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "total = 0\n",
    "cnt = 0\n",
    "def lstm_file(data_type,date_no,n_lag=1,type='I'):\n",
    "    global total, cnt\n",
    "    df=get_dataframe(data_type,date_no)\n",
    "    rf,scaler=fine_preprocess(data_type,date_no,n_lag,type)\n",
    "    train_X,train_y,test_X,test_y=split_data(rf)\n",
    "    model=train(data_type,date_no,train_X,train_y,test_X,test_y,n_lag)\n",
    "    inv_y, inv_yhat=predict(data_type,date_no,model,scaler,train_X,train_y,test_X,test_y,n_lag)\n",
    "    res=cal_rmse(inv_y, inv_yhat)\n",
    "    #print(res)\n",
    "    total = total+res\n",
    "    cnt = cnt+1\n",
    "    return res\n",
    "    #roc_auc(inv_y, inv_yhat)\n",
    "file_path = \"result.txt\"\n",
    "with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "    pass\n",
    "for i in range(4):\n",
    "    number_to_append = 42\n",
    "    for root, dirs, files in os.walk('Shanghai_T1DM'):\n",
    "            for file in files:\n",
    "                res =lstm_file('Shanghai_T1DM',file,i,'I')\n",
    "                with open(file_path, \"a\", encoding=\"utf-8\") as file:\n",
    "                    file.write(f\"I,{i} step,{res}\\n\")\n",
    "\n",
    "    with open(file_path, \"a\", encoding=\"utf-8\") as file:\n",
    "        file.write(f\"Summary:I,{i} step,{total/cnt}\\n\")\n",
    "    total = 0\n",
    "    cnt = 0\n",
    "    for root, dirs, files in os.walk('Shanghai_T2DM'):\n",
    "            for file in files:\n",
    "                res =lstm_file('Shanghai_T2DM',file,i,'II')\n",
    "                with open(file_path, \"a\", encoding=\"utf-8\") as file:\n",
    "                    file.write(f\"II,{i} step,{res}\\n\")\n",
    "    with open(file_path, \"a\", encoding=\"utf-8\") as file:\n",
    "        file.write(f\"Summary:II,{i} step,{total/cnt}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2821e41-d099-4018-a615-c170154c71fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
